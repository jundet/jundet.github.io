<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    

    

    



    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    
    
    
    <title>PyTorch 分布式 | GISTime</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="深度学习,PyTorch">
    <meta name="description" content="PyTorch分布式训练目前分布式训练的方法中主要分为数据并行与模型并行两种方法，其中在大多实验中以数据并行为主。模型并行主要存在于工业应用中，一般模型较大、训练数据是学术实验中训练数据的几何倍数。">
<meta name="keywords" content="深度学习,PyTorch">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch 分布式">
<meta property="og:url" content="http://www.gistime.cn/2019/11/11/distribute/index.html">
<meta property="og:site_name" content="GISTime">
<meta property="og:description" content="PyTorch分布式训练目前分布式训练的方法中主要分为数据并行与模型并行两种方法，其中在大多实验中以数据并行为主。模型并行主要存在于工业应用中，一般模型较大、训练数据是学术实验中训练数据的几何倍数。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://www.gistime.cn/2019/11/11/distribute/1.png">
<meta property="og:image" content="http://www.gistime.cn/2019/11/11/distribute/2.png">
<meta property="og:updated_time" content="2019-11-12T12:33:18.919Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PyTorch 分布式">
<meta name="twitter:description" content="PyTorch分布式训练目前分布式训练的方法中主要分为数据并行与模型并行两种方法，其中在大多实验中以数据并行为主。模型并行主要存在于工业应用中，一般模型较大、训练数据是学术实验中训练数据的几何倍数。">
<meta name="twitter:image" content="http://www.gistime.cn/2019/11/11/distribute/1.png">
    
        <link rel="alternate" type="application/atom+xml" title="GISTime" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/gistime.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="http://www.gistime.cn" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/gistime.png">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">GISTime</h5>
          <!-- <a href="mailto:GISTime for everyone" title="GISTime for everyone" class="mail">GISTime for everyone</a> -->
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                分类
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">PyTorch 分布式</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">PyTorch 分布式</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-11-11T08:27:55.000Z" itemprop="datePublished" class="page-time">
  2019-11-11
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/PyTorch/">PyTorch</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#PyTorch分布式训练"><span class="post-toc-text">PyTorch分布式训练</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#数据并行"><span class="post-toc-text">数据并行</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#DataParallel"><span class="post-toc-text">DataParallel</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#distributed"><span class="post-toc-text">distributed</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1、distribute的实现需要与多进行搭配"><span class="post-toc-text">1、distribute的实现需要与多进行搭配</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2、模型复制"><span class="post-toc-text">2、模型复制</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3、数据划分"><span class="post-toc-text">3、数据划分</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#模型并行"><span class="post-toc-text">模型并行</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-distribute"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">PyTorch 分布式</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-11-11 16:27:55" datetime="2019-11-11T08:27:55.000Z"  itemprop="datePublished">2019-11-11</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/PyTorch/">PyTorch</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="PyTorch分布式训练"><a href="#PyTorch分布式训练" class="headerlink" title="PyTorch分布式训练"></a>PyTorch分布式训练</h1><p>目前分布式训练的方法中主要分为数据并行与模型并行两种方法，其中在大多实验中以数据并行为主。模型并行主要存在于工业应用中，一般模型较大、训练数据是学术实验中训练数据的几何倍数。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2019/11/11/distribute/1.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure></p>
<h2 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h2><p>PyTorch中实现数据并行的方法主要有DataParallel 和distributed 两种方法。</p>
<h3 id="DataParallel"><a href="#DataParallel" class="headerlink" title="DataParallel"></a>DataParallel</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.DataParallel(Module)</span><br></pre></td></tr></table></figure>
<p>DataParallel可以实现数据的并行训练，默认会按照设定的batch size将数据平均分配到每个GPU上，假设在工作站A上共有4张GPU，设定batch size=32，此时每张卡上将会获得 size=32/4 =8 的数据量。</p>
<p>在前向传播中每张卡将分别计算前向传播的参数，在反向传播过程中会将梯度汇集到一张卡上计算其梯度下降。一般会将数据汇集到原始GPU中，因此会产生GPU负载不均衡的问题，常常会出现第一张卡会满负载而其他卡利用率仅为一半的情况。</p>
<p>盗张图 <a href="https://www.jianshu.com/p/9e36e5e36638?utm_source=oschina-app" target="_blank" rel="noopener">原图</a></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2019/11/11/distribute/2.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>DataParallel(Module) <a href="https://pytorch.org/docs/1.0.0/nn.html?highlight=dataparallel#torch.nn.DataParallel" target="_blank" rel="noopener">官方文档</a> <a href="https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html?highlight=dataparallel" target="_blank" rel="noopener">官方示例</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">This container parallelizes the application of the given :attr:`module` bysplitting the input across the specified devices by chunking in the batchdimension (other objects will be copied once per device). In the forwardpass, the module is replicated on each device, and each replica handles aportion of the input. During the backwards pass, gradients from each replicaare summed into the original module.</span><br></pre></td></tr></table></figure>
<p>实现方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">model = Net()</span><br><span class="line"><span class="comment"># 可直接指定需要使用GPU编号</span></span><br><span class="line">model = nn.DataParalle(model.cuda(), device_ids=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># 或者使用CUDA_VISIBLE_DEVICES指定</span></span><br><span class="line"><span class="comment"># os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'</span></span><br><span class="line"><span class="comment"># model = nn.DataParallel(model).cuda()</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch_idx, (imgs, ids) <span class="keyword">in</span> enumerate(trainloader):</span><br><span class="line">    <span class="comment"># 数据也要由内存转到GPU内存中</span></span><br><span class="line">    imgs, ids = imgs.cuda(), ids.cuda()</span><br><span class="line">    feature = model(imgs)</span><br><span class="line">    loss = criterion_xent(feature, ids)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>该使用方法在GPUs&lt;8时效果较好，在GPU数据较多时官方推荐使用多进程（multiprocessing）实现</p>
<h3 id="distributed"><a href="#distributed" class="headerlink" title="distributed"></a>distributed</h3><p>多机（节点）多卡训练 <a href="https://pytorch.org/docs/stable/distributed.html?highlight=distribut#module-torch.distributed" target="_blank" rel="noopener">官方函数说明</a> <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" target="_blank" rel="noopener">官方教程</a></p>
<p>ImageNet数据集是一个里程碑式的数据集，很多模型都是此数据集上进行预训练然后进行到其他数据集进行训练。如何更快的使用此数据集训练模型，推荐阅读腾讯的<a href="https://blog.csdn.net/Tencent_TEG/article/details/81295158" target="_blank" rel="noopener">4分钟训练ImageNet</a></p>
<p>在感叹腾讯的实力（GPU数量不是一般小实验室搞得起的，技术很强）之余，使用PyTorch在mnist数据集上进行小试牛刀。mnist在练习使用PyTorch时很有帮助，但在此处使用多机多卡进行训练时可能还不如单张卡。其主要原为mnist数据集本身很小，其训练时间短，多机多卡训练需要对数据进行划分并复制到不同的GPU上，划分所需的时间可能比真正训练的时间长。因此想要体现出多机多卡的优势请选择较大的模型和数据集，并将Batch size设置的大一些。</p>
<p>distributed 数据并行的几个细节</p>
<h4 id="1、distribute的实现需要与多进行搭配"><a href="#1、distribute的实现需要与多进行搭配" class="headerlink" title="1、distribute的实现需要与多进行搭配"></a>1、distribute的实现需要与多进行搭配</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line">mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))</span><br><span class="line"><span class="comment"># main_worker 是训练的具体函数，下面将进行介绍 </span></span><br><span class="line"><span class="comment"># nprocs 每个节点上GPU的个数</span></span><br></pre></td></tr></table></figure>
<h4 id="2、模型复制"><a href="#2、模型复制" class="headerlink" title="2、模型复制"></a>2、模型复制</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])</span><br><span class="line"><span class="comment"># 使用 DistributedDataParallel 函数将模型复制到GPU中</span></span><br></pre></td></tr></table></figure>
<h4 id="3、数据划分"><a href="#3、数据划分" class="headerlink" title="3、数据划分"></a>3、数据划分</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = datasets.MNIST(<span class="string">'data'</span>, train=<span class="keyword">True</span>, download=<span class="keyword">True</span>,</span><br><span class="line">                                   transform=transforms.Compose([</span><br><span class="line">                                       transforms.ToTensor(),</span><br><span class="line">                                       transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                                   ]))</span><br><span class="line"><span class="keyword">if</span> args.distributed:</span><br><span class="line">    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)</span><br><span class="line">    <span class="comment"># 使用DistributedSampler对数据集进行划分，例如batch size = 100，GPU=4，每张卡将分配到batch size = 25，此方法使用平均划分的方法，如有个性化需求需要自行实现数据划分</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    train_sampler = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    train_dataset, batch_size=args.batch_size, shuffle=(train_sampler <span class="keyword">is</span> <span class="keyword">None</span>),</span><br><span class="line">    num_workers=args.workers, pin_memory=<span class="keyword">True</span>, sampler=train_sampler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_sampler 为选取器，从train_dataset中按照train_sampler规则进行选取</span></span><br></pre></td></tr></table></figure>
<p>完整代码请看mnist_train.py，此代码根据<a href="https://github.com/pytorch/examples/tree/master/imagenet" target="_blank" rel="noopener">官方示例</a>改写，启动方式与官方示例的方式相同</p>
<p>Github代码示例</p>
<p>官方示例1：<a href="https://github.com/seba-1511/dist_tuto.pth" target="_blank" rel="noopener">https://github.com/seba-1511/dist_tuto.pth</a></p>
<p>官方示例2：<a href="https://github.com/pytorch/examples/tree/master/imagenet" target="_blank" rel="noopener">https://github.com/pytorch/examples/tree/master/imagenet</a></p>
<p><a href="https://github.com/yangkky/distributed_tutorial" target="_blank" rel="noopener">https://github.com/yangkky/distributed_tutorial</a></p>
<p>其他介绍：</p>
<p><a href="https://blog.csdn.net/zwqjoy/article/details/89415933" target="_blank" rel="noopener">https://blog.csdn.net/zwqjoy/article/details/89415933</a></p>
<p><a href="https://blog.csdn.net/m0_38008956/article/details/86559432" target="_blank" rel="noopener">https://blog.csdn.net/m0_38008956/article/details/86559432</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/68717029" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/68717029</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/38949622" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38949622</a></p>
<p><a href="https://blog.csdn.net/u010557442/article/details/79431520" target="_blank" rel="noopener">https://blog.csdn.net/u010557442/article/details/79431520</a></p>
<h2 id="模型并行"><a href="#模型并行" class="headerlink" title="模型并行"></a>模型并行</h2><p><a href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html" target="_blank" rel="noopener">官方示例</a></p>
<p>基本用法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToyModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ToyModel, self).__init__()</span><br><span class="line">        <span class="comment"># 将net1复制到GPU0上进行计算</span></span><br><span class="line">        self.net1 = torch.nn.Linear(<span class="number">10</span>, <span class="number">10</span>).to(<span class="string">'cuda:0'</span>)</span><br><span class="line">        self.relu = torch.nn.ReLU()</span><br><span class="line">        <span class="comment"># 将net2复制到GPU1上进行计算</span></span><br><span class="line">        self.net2 = torch.nn.Linear(<span class="number">10</span>, <span class="number">5</span>).to(<span class="string">'cuda:1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.relu(self.net1(x.to(<span class="string">'cuda:0'</span>)))</span><br><span class="line">        <span class="keyword">return</span> self.net2(x.to(<span class="string">'cuda:1'</span>))</span><br><span class="line">    </span><br><span class="line">model = ToyModel()</span><br><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">outputs = model(torch.randn(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">labels = torch.randn(<span class="number">20</span>, <span class="number">5</span>).to(<span class="string">'cuda:1'</span>)</span><br><span class="line">loss_fn(outputs, labels).backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>上述代码中的net1和net1既可以简单的一层网络也可以是nn.Sequential生成的网络片段。这种基本方法会将输入数据先在GPU0上进行训练，然后在转移到GPU1上进行训练，可以发现，无论何时都只有一张卡在计算而另一张卡在等待。为解决此问题，PyTorch提供了Pipelining Inputs（流水线）的方法实现模型并行。</p>
<p>假设Batch size = 120。按照split_size=20对batch size进行进一步划分。当PyTorch异步启动CUDA操作时，该实现无需生成多个线程即可实现并发。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PipelineParallelResNet50</span><span class="params">(ModelParallelResNet50)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, split_size=<span class="number">20</span>, *args, **kwargs)</span>:</span></span><br><span class="line">        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)</span><br><span class="line">        self.split_size = split_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        splits = iter(x.split(self.split_size, dim=<span class="number">0</span>))</span><br><span class="line">        s_next = next(splits)</span><br><span class="line">        s_prev = self.seq1(s_next).to(<span class="string">'cuda:1'</span>)</span><br><span class="line">        ret = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> s_next <span class="keyword">in</span> splits:</span><br><span class="line">            <span class="comment"># A. s_prev runs on cuda:1</span></span><br><span class="line">            s_prev = self.seq2(s_prev)</span><br><span class="line">            ret.append(self.fc(s_prev.view(s_prev.size(<span class="number">0</span>), <span class="number">-1</span>)))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># B. s_next runs on cuda:0, which can run concurrently with A</span></span><br><span class="line">            s_prev = self.seq1(s_next).to(<span class="string">'cuda:1'</span>)</span><br><span class="line"></span><br><span class="line">        s_prev = self.seq2(s_prev)</span><br><span class="line">        ret.append(self.fc(s_prev.view(s_prev.size(<span class="number">0</span>), <span class="number">-1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.cat(ret)</span><br></pre></td></tr></table></figure>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2019-11-12T12:33:18.919Z" itemprop="dateUpdated">2019-11-12 20:33:18</time>
</span><br>


        
    </div>
    
    <footer>
        <a href="http://www.gistime.cn">
            <img src="/img/gistime.png" alt="GISTime">
            GISTime
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PyTorch/">PyTorch</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://www.gistime.cn/2019/11/11/distribute/&title=《PyTorch 分布式》 — GISTime&pic=http://www.gistime.cn/img/gistime.png" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://www.gistime.cn/2019/11/11/distribute/&title=《PyTorch 分布式》 — GISTime&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://www.gistime.cn/2019/11/11/distribute/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《PyTorch 分布式》 — GISTime&url=http://www.gistime.cn/2019/11/11/distribute/&via=http://www.gistime.cn" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://www.gistime.cn/2019/11/11/distribute/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/11/14/ldapca/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">LDA与PDA</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/10/22/regularization/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">深度学习中的正则化</h4>
      </a>
    </div>
  
</nav>



    

















</article>



</div>

        <footer class="footer">
    <!-- <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div> -->
    <div class="bottom">
        <p><span><a href="http://www.gistime.cn/" target="_blank">GISTime</a> &copy; 2017 - 2019</span>
            <span>
                
                <a href="http://www.miitbeian.gov.cn/" target="_blank">鲁ICP备17010437号</a><br>
                
                <!-- Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a> -->
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://www.gistime.cn/2019/11/11/distribute/&title=《PyTorch 分布式》 — GISTime&pic=http://www.gistime.cn/img/gistime.png" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://www.gistime.cn/2019/11/11/distribute/&title=《PyTorch 分布式》 — GISTime&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://www.gistime.cn/2019/11/11/distribute/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《PyTorch 分布式》 — GISTime&url=http://www.gistime.cn/2019/11/11/distribute/&via=http://www.gistime.cn" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://www.gistime.cn/2019/11/11/distribute/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=http://www.gistime.cn/2019/11/11/distribute/" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: false };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>






<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'GISTime';
            clearTimeout(titleTime);
        } else {
            document.title = 'GISTime';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
